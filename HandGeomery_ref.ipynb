{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hand image preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#HAND IMAGE PREPROCESSING, NOT LINNAN'S APPROACH WHICH USES SURF\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from operator import itemgetter\n",
    "import glob\n",
    "\n",
    "def getHandMask(image):\n",
    "    cropped = cv2.imread(image)[:, 20:140]\n",
    "    rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
    "    hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # define range of hand color in HSV\n",
    "    lower_hand = np.array([0,.2*255,50])\n",
    "    upper_hand = np.array([40,.65*255,255])\n",
    "\n",
    "    mask = cv2.inRange(hsv, lower_hand, upper_hand)\n",
    "    mask = cv2.blur(mask, (3, 3))\n",
    "\n",
    "    return mask\n",
    "\n",
    "def findHandCountours(image):\n",
    "    mask = getHandMask(image)\n",
    "    edges = cv2.Canny(mask, 50,100)\n",
    "\n",
    "    im2,contours,hierarchy = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_TC89_KCOS)\n",
    "\n",
    "    contoursAreaIdx = [(idx, cv2.contourArea(cnt)) for idx, cnt in enumerate(contours)]\n",
    "    contoursAreaIdx.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "    image = np.zeros(mask.shape, np.uint8)\n",
    "    cv2.drawContours(image, contours, contoursAreaIdx[0][0], (255,255,255), 1)\n",
    "\n",
    "\n",
    "def findHandConnectedComponents(image):\n",
    "    mask = getHandMask(image)\n",
    "\n",
    "    num, labels, stats, centroids = cv2.connectedComponentsWithStats(mask, 8, cv2.CV_32S)\n",
    "\n",
    "    connectedAreas = sorted([(idx, stat[cv2.CC_STAT_AREA]) for idx, stat in enumerate(stats.tolist())], key=itemgetter(1), reverse=True)\n",
    "    #plt.imshow(mask)\n",
    "\n",
    "    maxConnectedAreaLabel = connectedAreas[1][0]\n",
    "    maxConnected = [[255 if e == maxConnectedAreaLabel  else 0 for e in row] for row in labels]\n",
    "    \n",
    "    img = np.array(maxConnected)\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.gray()\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import os, glob\n",
    "import glob\n",
    "import csv\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array, array_to_img\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Conv2D, Dropout, MaxPooling2D, Reshape, Activation, Flatten, LeakyReLU, Add, BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "height = 100\n",
    "width = 120\n",
    "n_channels = 3\n",
    "batch_size = 1\n",
    "n_train_images = 60\n",
    "total_images = 74\n",
    "num_images = 0\n",
    "\n",
    "train_hands = np.empty((n_train_images, height, width, n_channels), dtype=np.float32)\n",
    "test_hands = np.empty((total_images - n_train_images, height, width, n_channels), dtype=np.float32)\n",
    "all_hands = np.empty((total_images, height, width, n_channels), dtype=np.float32)\n",
    "\n",
    "#f = '../raw_data_small/001_HandPhoto_left_01.jpg'\n",
    "for x in glob.glob('processed/*.jpg'):\n",
    "    img = img_to_array(load_img(x))/255\n",
    "    all_hands[num_images] = img\n",
    "    if num_images >= n_train_images:\n",
    "        test_hands[num_images - n_train_images] = img\n",
    "    else:\n",
    "        train_hands[num_images] = img\n",
    "    num_images += 1\n",
    "    \n",
    "train_labels = np.empty((n_train_images, 1), dtype=np.float32)\n",
    "test_labels = np.empty((total_images - n_train_images, 1), dtype=np.float32)\n",
    "all_labels = np.empty((total_images, 1), dtype=np.float32)\n",
    "\n",
    "with open('trainTargets.csv', 'rb') as csvfile:\n",
    "    targets = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
    "    next(targets)\n",
    "    i = 0\n",
    "    for row in targets:\n",
    "        all_labels[i] = row[1]\n",
    "        if (i >= n_train_images):\n",
    "            test_labels[i - n_train_images] = row[1]\n",
    "        else:\n",
    "            train_labels[i] = row[1]\n",
    "        i +=1\n",
    "\n",
    "def vanilla_conv():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape = (height, width, n_channels)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def residual_block(input_layer):\n",
    "    layer = input_layer\n",
    "    layer = Conv2D(64, (4, 4), padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = LeakyReLU()(layer)\n",
    "    layer = Conv2D(64, (4,4), padding='same')(layer)\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Add()([input_layer, layer])\n",
    "    layer = LeakyReLU()(layer)\n",
    "    return layer\n",
    "\n",
    "def residual_conv(input_layer):\n",
    "    x = Conv2D(64, (4, 4), padding='same', input_shape=(height, width, n_channels))(input_layer)\n",
    "    #https://arxiv.org/abs/1502.03167\n",
    "    x = BatchNormalization()(x)\n",
    "    # Leaky ReLU. Leaky ReLUs are one attempt to fix the “dying ReLU” problem. \n",
    "    # Instead of the function being zero when x < 0, a leaky ReLU will instead \n",
    "    # have a small negative slope (of 0.01, or so). \n",
    "    x = LeakyReLU()(x)\n",
    "    x = residual_block(x)\n",
    "    x = residual_block(x)\n",
    "    x = residual_block(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    return x\n",
    "\n",
    "#Resnet Approach w/ exploding gradient issue\n",
    "#input_layer = Input(shape=(height, width, n_channels))\n",
    "#x = residual_conv(input_layer)\n",
    "#pred = Dense(1, activation='relu')(x)\n",
    "\n",
    "#model = Model(inputs=input_layer, outputs=pred)\n",
    "#print model.summary()\n",
    "\n",
    "model = vanilla_conv()\n",
    "model.compile(loss='mse', optimizer='adam', )\n",
    "\n",
    "tensorboard = TensorBoard(log_dir=\"/home/ubuntu/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 66 samples, validate on 8 samples\n",
      "Epoch 1/5000\n",
      "66/66 [==============================] - 1s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 2/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 3/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 4/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 5/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 6/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 7/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 8/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 9/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 10/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 11/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 12/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 13/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 14/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 15/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 16/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 17/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 18/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 19/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 20/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 21/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 22/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 23/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 24/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 25/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 26/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 27/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 28/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 29/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 30/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 31/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 32/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 33/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 34/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 35/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 36/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 37/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 38/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 39/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 40/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 41/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 42/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 43/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 44/5000\n",
      "66/66 [==============================] - 0s - loss: 51.3693 - val_loss: 54.2276\n",
      "Epoch 45/5000\n",
      "66/66 [==============================] - 0s - loss: 43.7863 - val_loss: 28.4958\n",
      "Epoch 46/5000\n",
      "66/66 [==============================] - 0s - loss: 6.5780 - val_loss: 31.3888\n",
      "Epoch 47/5000\n",
      "66/66 [==============================] - 0s - loss: 6.7235 - val_loss: 27.0475\n",
      "Epoch 48/5000\n",
      "66/66 [==============================] - 0s - loss: 4.0533 - val_loss: 24.3050\n",
      "Epoch 49/5000\n",
      "66/66 [==============================] - 0s - loss: 3.1004 - val_loss: 14.2341\n",
      "Epoch 50/5000\n",
      "66/66 [==============================] - 0s - loss: 1.4005 - val_loss: 7.6852\n",
      "Epoch 51/5000\n",
      "66/66 [==============================] - 0s - loss: 2.1930 - val_loss: 9.6335\n",
      "Epoch 52/5000\n",
      "66/66 [==============================] - 0s - loss: 1.2807 - val_loss: 12.8861\n",
      "Epoch 53/5000\n",
      "66/66 [==============================] - 0s - loss: 0.8849 - val_loss: 12.1629\n",
      "Epoch 54/5000\n",
      "66/66 [==============================] - 0s - loss: 0.9043 - val_loss: 8.1740\n",
      "Epoch 55/5000\n",
      "66/66 [==============================] - 0s - loss: 1.0414 - val_loss: 9.8433\n",
      "Epoch 56/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7974 - val_loss: 12.3161\n",
      "Epoch 57/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7604 - val_loss: 10.0870\n",
      "Epoch 58/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7433 - val_loss: 9.7146\n",
      "Epoch 59/5000\n",
      "66/66 [==============================] - 0s - loss: 0.8750 - val_loss: 10.0293\n",
      "Epoch 60/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6959 - val_loss: 12.0014\n",
      "Epoch 61/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7438 - val_loss: 12.7489\n",
      "Epoch 62/5000\n",
      "66/66 [==============================] - 0s - loss: 0.8457 - val_loss: 8.9161\n",
      "Epoch 63/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7624 - val_loss: 9.2399\n",
      "Epoch 64/5000\n",
      "66/66 [==============================] - 0s - loss: 0.8580 - val_loss: 9.4680\n",
      "Epoch 65/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7144 - val_loss: 8.9827\n",
      "Epoch 66/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5909 - val_loss: 11.7879\n",
      "Epoch 67/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6831 - val_loss: 12.7856\n",
      "Epoch 68/5000\n",
      "66/66 [==============================] - 0s - loss: 0.9150 - val_loss: 10.2481\n",
      "Epoch 69/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5967 - val_loss: 7.9668\n",
      "Epoch 70/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5192 - val_loss: 9.7383\n",
      "Epoch 71/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4620 - val_loss: 9.2104\n",
      "Epoch 72/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5752 - val_loss: 8.3973\n",
      "Epoch 73/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6645 - val_loss: 6.8287\n",
      "Epoch 74/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6048 - val_loss: 6.9560\n",
      "Epoch 75/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7275 - val_loss: 7.3927\n",
      "Epoch 76/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7518 - val_loss: 7.5235\n",
      "Epoch 77/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5785 - val_loss: 8.5706\n",
      "Epoch 78/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4869 - val_loss: 7.5629\n",
      "Epoch 79/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5458 - val_loss: 7.6242\n",
      "Epoch 80/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4789 - val_loss: 5.7871\n",
      "Epoch 81/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5839 - val_loss: 5.7468\n",
      "Epoch 82/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6039 - val_loss: 6.9573\n",
      "Epoch 83/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5903 - val_loss: 10.5588\n",
      "Epoch 84/5000\n",
      "66/66 [==============================] - 0s - loss: 0.7620 - val_loss: 7.7052\n",
      "Epoch 85/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6185 - val_loss: 7.2008\n",
      "Epoch 86/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5735 - val_loss: 6.8145\n",
      "Epoch 87/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5413 - val_loss: 5.7159\n",
      "Epoch 88/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6420 - val_loss: 6.3603\n",
      "Epoch 89/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5428 - val_loss: 9.8071\n",
      "Epoch 90/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6339 - val_loss: 5.1179\n",
      "Epoch 91/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4125 - val_loss: 6.8227\n",
      "Epoch 92/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4639 - val_loss: 6.2904\n",
      "Epoch 93/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5032 - val_loss: 7.0838\n",
      "Epoch 94/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4346 - val_loss: 6.2277\n",
      "Epoch 95/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4479 - val_loss: 6.2856\n",
      "Epoch 96/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4248 - val_loss: 6.3619\n",
      "Epoch 97/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5032 - val_loss: 6.2112\n",
      "Epoch 98/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5467 - val_loss: 3.3338\n",
      "Epoch 99/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6256 - val_loss: 5.3307\n",
      "Epoch 100/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4599 - val_loss: 6.2263\n",
      "Epoch 101/5000\n",
      "66/66 [==============================] - 0s - loss: 0.6471 - val_loss: 6.0528\n",
      "Epoch 102/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5085 - val_loss: 6.6014\n",
      "Epoch 103/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3302 - val_loss: 6.7151\n",
      "Epoch 104/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4261 - val_loss: 6.0052\n",
      "Epoch 105/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5204 - val_loss: 5.8443\n",
      "Epoch 106/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4253 - val_loss: 4.5085\n",
      "Epoch 107/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3532 - val_loss: 4.9260\n",
      "Epoch 108/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3068 - val_loss: 5.1708\n",
      "Epoch 109/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4346 - val_loss: 4.4777\n",
      "Epoch 110/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5195 - val_loss: 5.9873\n",
      "Epoch 111/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4342 - val_loss: 6.7034\n",
      "Epoch 112/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4207 - val_loss: 6.4609\n",
      "Epoch 113/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4208 - val_loss: 4.9599\n",
      "Epoch 114/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3937 - val_loss: 3.0849\n",
      "Epoch 115/5000\n",
      "66/66 [==============================] - 0s - loss: 0.9152 - val_loss: 5.1654\n",
      "Epoch 116/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3988 - val_loss: 7.9091\n",
      "Epoch 117/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5228 - val_loss: 7.5389\n",
      "Epoch 118/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5303 - val_loss: 5.1258\n",
      "Epoch 119/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4023 - val_loss: 4.8243\n",
      "Epoch 120/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3225 - val_loss: 5.4524\n",
      "Epoch 121/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4243 - val_loss: 5.9218\n",
      "Epoch 122/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3149 - val_loss: 5.0678\n",
      "Epoch 123/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3206 - val_loss: 5.8682\n",
      "Epoch 124/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3283 - val_loss: 4.7494\n",
      "Epoch 125/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3792 - val_loss: 5.2462\n",
      "Epoch 126/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4181 - val_loss: 6.4862\n",
      "Epoch 127/5000\n",
      "66/66 [==============================] - 0s - loss: 0.5672 - val_loss: 4.4101\n",
      "Epoch 128/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4204 - val_loss: 3.8348\n",
      "Epoch 129/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4637 - val_loss: 4.3941\n",
      "Epoch 130/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3059 - val_loss: 5.9834\n",
      "Epoch 131/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3839 - val_loss: 5.1806\n",
      "Epoch 132/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2333 - val_loss: 5.1908\n",
      "Epoch 133/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3054 - val_loss: 4.9699\n",
      "Epoch 134/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3982 - val_loss: 4.3465\n",
      "Epoch 135/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3749 - val_loss: 3.4770\n",
      "Epoch 136/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3830 - val_loss: 5.2777\n",
      "Epoch 137/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2693 - val_loss: 4.4921\n",
      "Epoch 138/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3036 - val_loss: 5.3075\n",
      "Epoch 139/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2438 - val_loss: 5.1614\n",
      "Epoch 140/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3289 - val_loss: 4.7296\n",
      "Epoch 141/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2027 - val_loss: 4.1214\n",
      "Epoch 142/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2892 - val_loss: 4.4270\n",
      "Epoch 143/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2897 - val_loss: 3.9480\n",
      "Epoch 144/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3289 - val_loss: 3.1437\n",
      "Epoch 145/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4997 - val_loss: 4.0494\n",
      "Epoch 146/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2251 - val_loss: 4.7968\n",
      "Epoch 147/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3059 - val_loss: 4.3813\n",
      "Epoch 148/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3383 - val_loss: 4.2881\n",
      "Epoch 149/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4966 - val_loss: 6.0219\n",
      "Epoch 150/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4581 - val_loss: 6.0191\n",
      "Epoch 151/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4160 - val_loss: 5.1624\n",
      "Epoch 152/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3660 - val_loss: 4.1145\n",
      "Epoch 153/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3297 - val_loss: 3.1523\n",
      "Epoch 154/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4916 - val_loss: 2.6438\n",
      "Epoch 155/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4387 - val_loss: 4.8758\n",
      "Epoch 156/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2818 - val_loss: 3.7808\n",
      "Epoch 157/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2989 - val_loss: 4.9257\n",
      "Epoch 158/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2836 - val_loss: 3.8391\n",
      "Epoch 159/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2622 - val_loss: 5.2375\n",
      "Epoch 160/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3131 - val_loss: 4.0495\n",
      "Epoch 161/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2226 - val_loss: 3.9831\n",
      "Epoch 162/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2457 - val_loss: 4.3832\n",
      "Epoch 163/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2144 - val_loss: 4.5606\n",
      "Epoch 164/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2656 - val_loss: 4.0847\n",
      "Epoch 165/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3024 - val_loss: 3.8481\n",
      "Epoch 166/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2170 - val_loss: 4.0907\n",
      "Epoch 167/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1995 - val_loss: 3.8107\n",
      "Epoch 168/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1791 - val_loss: 4.3777\n",
      "Epoch 169/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1984 - val_loss: 4.4084\n",
      "Epoch 170/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1993 - val_loss: 4.7548\n",
      "Epoch 171/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2109 - val_loss: 5.0125\n",
      "Epoch 172/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3294 - val_loss: 5.1019\n",
      "Epoch 173/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3381 - val_loss: 4.5743\n",
      "Epoch 174/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2722 - val_loss: 3.8661\n",
      "Epoch 175/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2775 - val_loss: 2.6285\n",
      "Epoch 176/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3324 - val_loss: 3.0857\n",
      "Epoch 177/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2520 - val_loss: 3.6085\n",
      "Epoch 178/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2652 - val_loss: 4.8349\n",
      "Epoch 179/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2276 - val_loss: 4.4440\n",
      "Epoch 180/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2689 - val_loss: 3.0952\n",
      "Epoch 181/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2515 - val_loss: 3.5599\n",
      "Epoch 182/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1940 - val_loss: 3.7995\n",
      "Epoch 183/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2020 - val_loss: 3.3851\n",
      "Epoch 184/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2309 - val_loss: 3.3456\n",
      "Epoch 185/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1879 - val_loss: 3.9511\n",
      "Epoch 186/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2350 - val_loss: 4.2835\n",
      "Epoch 187/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1892 - val_loss: 3.9237\n",
      "Epoch 188/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1969 - val_loss: 4.3624\n",
      "Epoch 189/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2165 - val_loss: 4.1537\n",
      "Epoch 190/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1628 - val_loss: 2.6313\n",
      "Epoch 191/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2534 - val_loss: 3.5488\n",
      "Epoch 192/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2328 - val_loss: 4.3699\n",
      "Epoch 193/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2401 - val_loss: 5.1328\n",
      "Epoch 194/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3321 - val_loss: 4.2183\n",
      "Epoch 195/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2179 - val_loss: 2.4711\n",
      "Epoch 196/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1992 - val_loss: 3.0924\n",
      "Epoch 197/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2630 - val_loss: 4.8030\n",
      "Epoch 198/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2185 - val_loss: 5.0219\n",
      "Epoch 199/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3741 - val_loss: 3.4559\n",
      "Epoch 200/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3626 - val_loss: 1.9803\n",
      "Epoch 201/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3399 - val_loss: 2.7118\n",
      "Epoch 202/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3022 - val_loss: 4.4516\n",
      "Epoch 203/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2735 - val_loss: 4.8803\n",
      "Epoch 204/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4076 - val_loss: 2.8014\n",
      "Epoch 205/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2272 - val_loss: 2.5517\n",
      "Epoch 206/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2877 - val_loss: 3.9986\n",
      "Epoch 207/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3260 - val_loss: 4.7942\n",
      "Epoch 208/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2773 - val_loss: 3.4286\n",
      "Epoch 209/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3109 - val_loss: 2.8151\n",
      "Epoch 210/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1590 - val_loss: 2.7067\n",
      "Epoch 211/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1967 - val_loss: 4.1488\n",
      "Epoch 212/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3006 - val_loss: 3.4866\n",
      "Epoch 213/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2540 - val_loss: 2.3156\n",
      "Epoch 214/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2267 - val_loss: 3.1554\n",
      "Epoch 215/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1813 - val_loss: 4.1775\n",
      "Epoch 216/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2057 - val_loss: 3.7415\n",
      "Epoch 217/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1813 - val_loss: 3.1770\n",
      "Epoch 218/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1284 - val_loss: 3.4929\n",
      "Epoch 219/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2166 - val_loss: 3.8779\n",
      "Epoch 220/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2169 - val_loss: 4.0644\n",
      "Epoch 221/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1932 - val_loss: 3.6957\n",
      "Epoch 222/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1380 - val_loss: 2.4689\n",
      "Epoch 223/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2075 - val_loss: 3.5425\n",
      "Epoch 224/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1810 - val_loss: 3.5885\n",
      "Epoch 225/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2068 - val_loss: 4.3021\n",
      "Epoch 226/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2629 - val_loss: 3.2548\n",
      "Epoch 227/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2649 - val_loss: 2.9976\n",
      "Epoch 228/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1665 - val_loss: 3.5156\n",
      "Epoch 229/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2955 - val_loss: 4.3609\n",
      "Epoch 230/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3259 - val_loss: 4.2297\n",
      "Epoch 231/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2453 - val_loss: 3.5885\n",
      "Epoch 232/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1246 - val_loss: 3.0028\n",
      "Epoch 233/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3269 - val_loss: 2.9774\n",
      "Epoch 234/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1276 - val_loss: 2.5407\n",
      "Epoch 235/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2685 - val_loss: 3.8348\n",
      "Epoch 236/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2369 - val_loss: 3.7636\n",
      "Epoch 237/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2958 - val_loss: 3.4672\n",
      "Epoch 238/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2246 - val_loss: 2.1012\n",
      "Epoch 239/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2008 - val_loss: 2.5759\n",
      "Epoch 240/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2422 - val_loss: 4.1892\n",
      "Epoch 241/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2138 - val_loss: 3.4878\n",
      "Epoch 242/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1221 - val_loss: 3.5358\n",
      "Epoch 243/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1535 - val_loss: 3.1271\n",
      "Epoch 244/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1795 - val_loss: 3.6371\n",
      "Epoch 245/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1710 - val_loss: 2.3096\n",
      "Epoch 246/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2471 - val_loss: 2.4840\n",
      "Epoch 247/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1977 - val_loss: 4.1244\n",
      "Epoch 248/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1882 - val_loss: 3.6538\n",
      "Epoch 249/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1540 - val_loss: 3.3573\n",
      "Epoch 250/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1699 - val_loss: 2.1899\n",
      "Epoch 251/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1269 - val_loss: 3.4605\n",
      "Epoch 252/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1437 - val_loss: 3.5198\n",
      "Epoch 253/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1594 - val_loss: 3.4157\n",
      "Epoch 254/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0931 - val_loss: 2.6930\n",
      "Epoch 255/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2040 - val_loss: 2.9217\n",
      "Epoch 256/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1278 - val_loss: 3.4958\n",
      "Epoch 257/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1718 - val_loss: 3.4192\n",
      "Epoch 258/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1431 - val_loss: 2.8385\n",
      "Epoch 259/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1177 - val_loss: 3.3628\n",
      "Epoch 260/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1526 - val_loss: 3.6397\n",
      "Epoch 261/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2678 - val_loss: 3.7747\n",
      "Epoch 262/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2415 - val_loss: 1.5758\n",
      "Epoch 263/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2656 - val_loss: 3.1557\n",
      "Epoch 264/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3833 - val_loss: 5.1844\n",
      "Epoch 265/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4692 - val_loss: 3.9585\n",
      "Epoch 266/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4173 - val_loss: 2.2041\n",
      "Epoch 267/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2082 - val_loss: 2.0700\n",
      "Epoch 268/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2381 - val_loss: 3.1915\n",
      "Epoch 269/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1511 - val_loss: 3.0736\n",
      "Epoch 270/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1502 - val_loss: 2.2735\n",
      "Epoch 271/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1096 - val_loss: 2.6785\n",
      "Epoch 272/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1346 - val_loss: 2.3602\n",
      "Epoch 273/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1637 - val_loss: 2.6109\n",
      "Epoch 274/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1317 - val_loss: 3.0664\n",
      "Epoch 275/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1024 - val_loss: 3.2577\n",
      "Epoch 276/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1121 - val_loss: 3.3749\n",
      "Epoch 277/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1438 - val_loss: 2.5724\n",
      "Epoch 278/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1551 - val_loss: 2.8495\n",
      "Epoch 279/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1250 - val_loss: 3.5592\n",
      "Epoch 280/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2028 - val_loss: 2.5465\n",
      "Epoch 281/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1403 - val_loss: 2.1663\n",
      "Epoch 282/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1873 - val_loss: 3.4862\n",
      "Epoch 283/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2227 - val_loss: 3.4781\n",
      "Epoch 284/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1631 - val_loss: 2.8182\n",
      "Epoch 285/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1289 - val_loss: 3.4566\n",
      "Epoch 286/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2067 - val_loss: 2.9597\n",
      "Epoch 287/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1807 - val_loss: 1.8339\n",
      "Epoch 288/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1462 - val_loss: 2.8201\n",
      "Epoch 289/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0990 - val_loss: 3.2622\n",
      "Epoch 290/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2484 - val_loss: 2.4434\n",
      "Epoch 291/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2345 - val_loss: 1.5810\n",
      "Epoch 292/5000\n",
      "66/66 [==============================] - 0s - loss: 0.4383 - val_loss: 3.2137\n",
      "Epoch 293/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2484 - val_loss: 3.1937\n",
      "Epoch 294/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1476 - val_loss: 2.1705\n",
      "Epoch 295/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2006 - val_loss: 2.2482\n",
      "Epoch 296/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1712 - val_loss: 2.8917\n",
      "Epoch 297/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1594 - val_loss: 2.4515\n",
      "Epoch 298/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1525 - val_loss: 2.3596\n",
      "Epoch 299/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1318 - val_loss: 2.6429\n",
      "Epoch 300/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1275 - val_loss: 1.9887\n",
      "Epoch 301/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1802 - val_loss: 2.4418\n",
      "Epoch 302/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1210 - val_loss: 2.2461\n",
      "Epoch 303/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1528 - val_loss: 2.4542\n",
      "Epoch 304/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0993 - val_loss: 2.0545\n",
      "Epoch 305/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1148 - val_loss: 2.2332\n",
      "Epoch 306/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0723 - val_loss: 2.4492\n",
      "Epoch 307/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1475 - val_loss: 2.2537\n",
      "Epoch 308/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1100 - val_loss: 2.5012\n",
      "Epoch 309/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1106 - val_loss: 2.5521\n",
      "Epoch 310/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1607 - val_loss: 2.3128\n",
      "Epoch 311/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1134 - val_loss: 3.4029\n",
      "Epoch 312/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1773 - val_loss: 3.2155\n",
      "Epoch 313/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1649 - val_loss: 1.9920\n",
      "Epoch 314/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1921 - val_loss: 2.1620\n",
      "Epoch 315/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1927 - val_loss: 2.8331\n",
      "Epoch 316/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1530 - val_loss: 2.7337\n",
      "Epoch 317/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1422 - val_loss: 1.8138\n",
      "Epoch 318/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1223 - val_loss: 2.5043\n",
      "Epoch 319/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1275 - val_loss: 2.5123\n",
      "Epoch 320/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1135 - val_loss: 2.7481\n",
      "Epoch 321/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1199 - val_loss: 2.0633\n",
      "Epoch 322/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1466 - val_loss: 2.6390\n",
      "Epoch 323/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1581 - val_loss: 2.9555\n",
      "Epoch 324/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1820 - val_loss: 3.2901\n",
      "Epoch 325/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1313 - val_loss: 1.2952\n",
      "Epoch 326/5000\n",
      "66/66 [==============================] - 0s - loss: 0.3603 - val_loss: 2.9324\n",
      "Epoch 327/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2079 - val_loss: 3.3002\n",
      "Epoch 328/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1564 - val_loss: 2.2401\n",
      "Epoch 329/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1582 - val_loss: 2.4145\n",
      "Epoch 330/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1159 - val_loss: 2.4644\n",
      "Epoch 331/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1225 - val_loss: 2.0849\n",
      "Epoch 332/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1034 - val_loss: 3.1819\n",
      "Epoch 333/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1821 - val_loss: 2.0465\n",
      "Epoch 334/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1670 - val_loss: 1.6650\n",
      "Epoch 335/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2211 - val_loss: 2.8489\n",
      "Epoch 336/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1115 - val_loss: 2.4952\n",
      "Epoch 337/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1116 - val_loss: 1.8473\n",
      "Epoch 338/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1389 - val_loss: 2.6115\n",
      "Epoch 339/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1052 - val_loss: 2.6018\n",
      "Epoch 340/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2055 - val_loss: 2.0449\n",
      "Epoch 341/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1425 - val_loss: 2.4288\n",
      "Epoch 342/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0846 - val_loss: 2.2135\n",
      "Epoch 343/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1127 - val_loss: 2.4863\n",
      "Epoch 344/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1415 - val_loss: 2.3573\n",
      "Epoch 345/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1393 - val_loss: 2.5664\n",
      "Epoch 346/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0824 - val_loss: 2.4591\n",
      "Epoch 347/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1002 - val_loss: 2.3292\n",
      "Epoch 348/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1014 - val_loss: 2.6003\n",
      "Epoch 349/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1138 - val_loss: 2.2379\n",
      "Epoch 350/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0855 - val_loss: 2.8305\n",
      "Epoch 351/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1417 - val_loss: 2.4966\n",
      "Epoch 352/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1340 - val_loss: 1.8750\n",
      "Epoch 353/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1505 - val_loss: 2.3062\n",
      "Epoch 354/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1984 - val_loss: 2.4765\n",
      "Epoch 355/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1173 - val_loss: 2.2107\n",
      "Epoch 356/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1195 - val_loss: 2.3967\n",
      "Epoch 357/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1079 - val_loss: 2.2351\n",
      "Epoch 358/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1163 - val_loss: 2.1087\n",
      "Epoch 359/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1150 - val_loss: 2.1076\n",
      "Epoch 360/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1082 - val_loss: 1.9593\n",
      "Epoch 361/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1466 - val_loss: 2.3453\n",
      "Epoch 362/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1350 - val_loss: 2.4879\n",
      "Epoch 363/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1098 - val_loss: 2.5133\n",
      "Epoch 364/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1480 - val_loss: 1.8813\n",
      "Epoch 365/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1296 - val_loss: 2.1809\n",
      "Epoch 366/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0991 - val_loss: 2.1272\n",
      "Epoch 367/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0902 - val_loss: 2.3747\n",
      "Epoch 368/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0778 - val_loss: 2.7267\n",
      "Epoch 369/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1650 - val_loss: 2.8151\n",
      "Epoch 370/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1495 - val_loss: 2.0088\n",
      "Epoch 371/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1001 - val_loss: 2.3681\n",
      "Epoch 372/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1181 - val_loss: 2.6047\n",
      "Epoch 373/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1201 - val_loss: 2.3849\n",
      "Epoch 374/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1083 - val_loss: 1.9167\n",
      "Epoch 375/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1636 - val_loss: 2.1398\n",
      "Epoch 376/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1713 - val_loss: 2.6736\n",
      "Epoch 377/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1101 - val_loss: 2.7904\n",
      "Epoch 378/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1386 - val_loss: 2.0680\n",
      "Epoch 379/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1009 - val_loss: 2.2859\n",
      "Epoch 380/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1093 - val_loss: 1.9867\n",
      "Epoch 381/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1402 - val_loss: 2.6595\n",
      "Epoch 382/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1331 - val_loss: 2.4009\n",
      "Epoch 383/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1085 - val_loss: 2.2534\n",
      "Epoch 384/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1123 - val_loss: 2.2899\n",
      "Epoch 385/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1036 - val_loss: 2.6832\n",
      "Epoch 386/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1029 - val_loss: 2.2758\n",
      "Epoch 387/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0958 - val_loss: 2.0899\n",
      "Epoch 388/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1108 - val_loss: 1.6994\n",
      "Epoch 389/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1122 - val_loss: 2.3581\n",
      "Epoch 390/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1639 - val_loss: 2.5535\n",
      "Epoch 391/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0836 - val_loss: 1.7362\n",
      "Epoch 392/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1667 - val_loss: 2.2943\n",
      "Epoch 393/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0942 - val_loss: 2.8408\n",
      "Epoch 394/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1159 - val_loss: 1.7218\n",
      "Epoch 395/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1055 - val_loss: 1.8875\n",
      "Epoch 396/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1031 - val_loss: 1.8475\n",
      "Epoch 397/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1306 - val_loss: 2.1355\n",
      "Epoch 398/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1024 - val_loss: 2.1208\n",
      "Epoch 399/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1188 - val_loss: 2.0798\n",
      "Epoch 400/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1468 - val_loss: 1.5571\n",
      "Epoch 401/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1709 - val_loss: 2.0152\n",
      "Epoch 402/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1306 - val_loss: 2.2795\n",
      "Epoch 403/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1013 - val_loss: 1.7306\n",
      "Epoch 404/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1061 - val_loss: 1.7987\n",
      "Epoch 405/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1333 - val_loss: 1.5902\n",
      "Epoch 406/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0979 - val_loss: 1.7588\n",
      "Epoch 407/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0839 - val_loss: 2.3174\n",
      "Epoch 408/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1482 - val_loss: 2.1404\n",
      "Epoch 409/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1069 - val_loss: 2.1514\n",
      "Epoch 410/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1216 - val_loss: 2.1345\n",
      "Epoch 411/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1614 - val_loss: 1.6806\n",
      "Epoch 412/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1036 - val_loss: 2.2269\n",
      "Epoch 413/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0937 - val_loss: 2.1277\n",
      "Epoch 414/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0881 - val_loss: 2.0565\n",
      "Epoch 415/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0855 - val_loss: 2.2546\n",
      "Epoch 416/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1020 - val_loss: 2.1037\n",
      "Epoch 417/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1035 - val_loss: 2.1880\n",
      "Epoch 418/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0761 - val_loss: 1.9655\n",
      "Epoch 419/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1397 - val_loss: 1.8797\n",
      "Epoch 420/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1719 - val_loss: 2.2881\n",
      "Epoch 421/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1140 - val_loss: 1.9078\n",
      "Epoch 422/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0886 - val_loss: 2.2718\n",
      "Epoch 423/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0955 - val_loss: 2.0970\n",
      "Epoch 424/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1066 - val_loss: 1.7689\n",
      "Epoch 425/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0775 - val_loss: 1.7252\n",
      "Epoch 426/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0919 - val_loss: 2.1319\n",
      "Epoch 427/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0786 - val_loss: 1.8509\n",
      "Epoch 428/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0820 - val_loss: 1.9102\n",
      "Epoch 429/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1145 - val_loss: 1.6603\n",
      "Epoch 430/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1239 - val_loss: 2.3605\n",
      "Epoch 431/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0861 - val_loss: 2.0612\n",
      "Epoch 432/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0980 - val_loss: 1.8190\n",
      "Epoch 433/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1281 - val_loss: 2.6289\n",
      "Epoch 434/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1877 - val_loss: 2.2397\n",
      "Epoch 435/5000\n",
      "66/66 [==============================] - 0s - loss: 0.2047 - val_loss: 1.1781\n",
      "Epoch 436/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1458 - val_loss: 2.0319\n",
      "Epoch 437/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1155 - val_loss: 2.4046\n",
      "Epoch 438/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1458 - val_loss: 1.6611\n",
      "Epoch 439/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1404 - val_loss: 1.4914\n",
      "Epoch 440/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1334 - val_loss: 2.2300\n",
      "Epoch 441/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1022 - val_loss: 1.7628\n",
      "Epoch 442/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1165 - val_loss: 2.0870\n",
      "Epoch 443/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1093 - val_loss: 2.2456\n",
      "Epoch 444/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0787 - val_loss: 2.2793\n",
      "Epoch 445/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1420 - val_loss: 1.6055\n",
      "Epoch 446/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1457 - val_loss: 1.5912\n",
      "Epoch 447/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1291 - val_loss: 1.7926\n",
      "Epoch 448/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0956 - val_loss: 1.7697\n",
      "Epoch 449/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1174 - val_loss: 2.1831\n",
      "Epoch 450/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1184 - val_loss: 1.8493\n",
      "Epoch 451/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1412 - val_loss: 1.6480\n",
      "Epoch 452/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1175 - val_loss: 2.2191\n",
      "Epoch 453/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1154 - val_loss: 2.1346\n",
      "Epoch 454/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1139 - val_loss: 2.0178\n",
      "Epoch 455/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1238 - val_loss: 1.8310\n",
      "Epoch 456/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1227 - val_loss: 1.2530\n",
      "Epoch 457/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0982 - val_loss: 1.5913\n",
      "Epoch 458/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0980 - val_loss: 1.8298\n",
      "Epoch 459/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0788 - val_loss: 1.8870\n",
      "Epoch 460/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0680 - val_loss: 1.6092\n",
      "Epoch 461/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1301 - val_loss: 1.7702\n",
      "Epoch 462/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0926 - val_loss: 2.2465\n",
      "Epoch 463/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1345 - val_loss: 1.6403\n",
      "Epoch 464/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1202 - val_loss: 1.6231\n",
      "Epoch 465/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0894 - val_loss: 1.9281\n",
      "Epoch 466/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0863 - val_loss: 1.1740\n",
      "Epoch 467/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1164 - val_loss: 1.7561\n",
      "Epoch 468/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0985 - val_loss: 1.5537\n",
      "Epoch 469/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1117 - val_loss: 1.8587\n",
      "Epoch 470/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1183 - val_loss: 1.6317\n",
      "Epoch 471/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0972 - val_loss: 1.6309\n",
      "Epoch 472/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0994 - val_loss: 1.3298\n",
      "Epoch 473/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1461 - val_loss: 1.8814\n",
      "Epoch 474/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1017 - val_loss: 1.7974\n",
      "Epoch 475/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1397 - val_loss: 1.3623\n",
      "Epoch 476/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1079 - val_loss: 1.3793\n",
      "Epoch 477/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0812 - val_loss: 1.6223\n",
      "Epoch 478/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0892 - val_loss: 1.5068\n",
      "Epoch 479/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1037 - val_loss: 1.6740\n",
      "Epoch 480/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0857 - val_loss: 1.5566\n",
      "Epoch 481/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0881 - val_loss: 1.6324\n",
      "Epoch 482/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1183 - val_loss: 1.7457\n",
      "Epoch 483/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0763 - val_loss: 1.5544\n",
      "Epoch 484/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1062 - val_loss: 1.4930\n",
      "Epoch 485/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1057 - val_loss: 1.7216\n",
      "Epoch 486/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0590 - val_loss: 1.6263\n",
      "Epoch 487/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0989 - val_loss: 1.7336\n",
      "Epoch 488/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0945 - val_loss: 1.7627\n",
      "Epoch 489/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1059 - val_loss: 1.9048\n",
      "Epoch 490/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0789 - val_loss: 1.4378\n",
      "Epoch 491/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0980 - val_loss: 1.5208\n",
      "Epoch 492/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1191 - val_loss: 1.9325\n",
      "Epoch 493/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1421 - val_loss: 1.7048\n",
      "Epoch 494/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0951 - val_loss: 1.3407\n",
      "Epoch 495/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1253 - val_loss: 1.7566\n",
      "Epoch 496/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1172 - val_loss: 1.9669\n",
      "Epoch 497/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1371 - val_loss: 2.0111\n",
      "Epoch 498/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1380 - val_loss: 1.8324\n",
      "Epoch 499/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0868 - val_loss: 1.2823\n",
      "Epoch 500/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1024 - val_loss: 1.5717\n",
      "Epoch 501/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0997 - val_loss: 1.1793\n",
      "Epoch 502/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0906 - val_loss: 1.4137\n",
      "Epoch 503/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0783 - val_loss: 1.4301\n",
      "Epoch 504/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1079 - val_loss: 1.5806\n",
      "Epoch 505/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0915 - val_loss: 1.6038\n",
      "Epoch 506/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0932 - val_loss: 1.4054\n",
      "Epoch 507/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0999 - val_loss: 1.1992\n",
      "Epoch 508/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1558 - val_loss: 1.6126\n",
      "Epoch 509/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0974 - val_loss: 1.6856\n",
      "Epoch 510/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1001 - val_loss: 1.5419\n",
      "Epoch 511/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1151 - val_loss: 1.2225\n",
      "Epoch 512/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0992 - val_loss: 1.6236\n",
      "Epoch 513/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0978 - val_loss: 1.5062\n",
      "Epoch 514/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1176 - val_loss: 1.1895\n",
      "Epoch 515/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1260 - val_loss: 1.5184\n",
      "Epoch 516/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1049 - val_loss: 1.3634\n",
      "Epoch 517/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0724 - val_loss: 1.4281\n",
      "Epoch 518/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0816 - val_loss: 1.3336\n",
      "Epoch 519/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0686 - val_loss: 1.3965\n",
      "Epoch 520/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0983 - val_loss: 1.5129\n",
      "Epoch 521/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0930 - val_loss: 1.5826\n",
      "Epoch 522/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1117 - val_loss: 1.2415\n",
      "Epoch 523/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1082 - val_loss: 1.2156\n",
      "Epoch 524/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0902 - val_loss: 1.3754\n",
      "Epoch 525/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0867 - val_loss: 1.0646\n",
      "Epoch 526/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1060 - val_loss: 1.8114\n",
      "Epoch 527/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0963 - val_loss: 1.7581\n",
      "Epoch 528/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0839 - val_loss: 1.1560\n",
      "Epoch 529/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0887 - val_loss: 1.4683\n",
      "Epoch 530/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0927 - val_loss: 1.8088\n",
      "Epoch 531/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0797 - val_loss: 1.4083\n",
      "Epoch 532/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0890 - val_loss: 1.2717\n",
      "Epoch 533/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0681 - val_loss: 1.5867\n",
      "Epoch 534/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0798 - val_loss: 1.3424\n",
      "Epoch 535/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0946 - val_loss: 1.4911\n",
      "Epoch 536/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0796 - val_loss: 1.4683\n",
      "Epoch 537/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1540 - val_loss: 1.3947\n",
      "Epoch 538/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0819 - val_loss: 1.7140\n",
      "Epoch 539/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0786 - val_loss: 1.6411\n",
      "Epoch 540/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0844 - val_loss: 1.2415\n",
      "Epoch 541/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0931 - val_loss: 1.8061\n",
      "Epoch 542/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1040 - val_loss: 1.6736\n",
      "Epoch 543/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1286 - val_loss: 1.2075\n",
      "Epoch 544/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0831 - val_loss: 1.5667\n",
      "Epoch 545/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0912 - val_loss: 1.4529\n",
      "Epoch 546/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1032 - val_loss: 1.8706\n",
      "Epoch 547/5000\n",
      "66/66 [==============================] - 0s - loss: 0.1155 - val_loss: 1.2943\n",
      "Epoch 548/5000\n",
      "66/66 [==============================] - 0s - loss: 0.0821 - val_loss: 1.4546\n",
      "Epoch 549/5000\n",
      "10/66 [===>..........................] - ETA: 0s - loss: 0.0855"
     ]
    }
   ],
   "source": [
    "model.fit(all_hands, all_labels, epochs=5000, batch_size=10, validation_split=.1, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'height' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-237c56e51497>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_vgg16_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_vgg16_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'height' is not defined"
     ]
    }
   ],
   "source": [
    "# Transfer Learning Approach From Keras\n",
    "\n",
    "from keras import applications\n",
    "from keras.layers import Input\n",
    "\n",
    "model_vgg16_conv = applications.vgg16.VGG16(weights='imagenet', include_top=False)\n",
    "model_vgg16_conv.summary()\n",
    "\n",
    "input = Input(shape=(height, width, n_channels))\n",
    "out = model_vgg16_conv(input)\n",
    "\n",
    "x = Flatten()(out)\n",
    "x = Dense(64, activation='relu')(x)\n",
    "x = Dense(1, activation='relu')(x)\n",
    "\n",
    "model = Model(inputs=input, outputs=x)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.summary()\n",
    "model.fit(train_hands, train_labels, epochs=1000, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('predict: ', 7.3440695) actual:  7.33522\n",
      "('predict: ', 6.7360468) actual:  6.76824\n",
      "('predict: ', 6.6889181) actual:  6.69317\n",
      "('predict: ', 6.681716) actual:  6.7571\n",
      "('predict: ', 8.0478287) actual:  8.03722\n",
      "('predict: ', 6.5520983) actual:  6.6638\n",
      "('predict: ', 7.0040674) actual:  8.44109\n",
      "('predict: ', 7.2573714) actual:  7.1412\n",
      "('predict: ', 7.3234377) actual:  6.81626\n",
      "('predict: ', 7.25103) actual:  7.41646\n",
      "('predict: ', 7.1030378) actual:  7.4375\n",
      "('predict: ', 7.2781258) actual:  7.68456\n",
      "('predict: ', 7.306385) actual:  7.42929\n",
      "('predict: ', 6.736938) actual:  6.36737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 6.736938]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img = np.empty((1, height, width, n_channels))\n",
    "\n",
    "for idx, image in enumerate(test_hands):\n",
    "    img[0] = image\n",
    "    print('predict: ', model.predict(img, batch_size=10)[0][0]), 'actual: ',  test_labels[idx][0]\n",
    "model.predict(img, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Manual Approach with a Linear Scan over Images and simple linear model to translate linear scan pixels to wrist widths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
